<launch>

 <!-- First launch the kinect2 bridge -->
  <include file="$(find kinect2_bridge)/launch/kinect2_bridge.launch" />

  <!-- run the tf transformation -->
  <node pkg="tf" type="static_transform_publisher" name="link1_broadcaster" args="0 0 1.4 1 0 0 0 base kinect2_base_link 100" />

  <!-- launch the ros perception launch file -->
  <!-- <include file="$(find lab_ros_perception)/launch/aruco_kinect2.launch" /> -->
  <!-- launch rosbridge server -->
  <include file="$(find rosbridge_server)/launch/rosbridge_websocket.launch" />
  <!-- launch the speech to next nodes -->
  <!-- <include file="$(find lab_ros_speech_to_text)/launch/google_stt.launch"/> -->
  <!-- launch Cozmo driver -->
  <!-- <include file="$(find cozmo_driver)/launch/base.launch" /> -->

  <!-- launch relay -->
  <node name="empathy_replay" pkg="bystander-Intervention-study" respawn="true" type="empathy_relay.py">
    <!-- local hack to allow us to include virtualenv into the equation -->
    <!--<env name="PYTHONPATH" value="$(env PYTHONPATH):/home/zhi/Dev/ros_ws/devel/lib/python2.7/dist-packages:/opt/ros/kinetic/lib/python2.7/dist-packages:/home/zhi/Dev/ENVs/cozmo_study/lib/python35.zip:/home/zhi/Dev/ENVs/cozmo_study/lib/python3.5:/home/zhi/Dev/ENVs/cozmo_study/lib/python3.5/plat-x86_64-linux-gnu:/home/zhi/Dev/ENVs/cozmo_study/lib/python3.5/lib-dynload:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/home/zhi/Dev/ENVs/cozmo_study/lib/python3.5/site-packages" /> -->
  </node>

  <include file="$(find audio_capture)/launch/capture.launch"/>

  <arg name="bag_path"/>

  <!-- rosbag recording -->
  <node pkg="rosbag" type="record" name="rosbag_status_record"
    args="--split --size=2048 -o $(arg bag_path)/bully.bag /rosout /kinect2/hd/image_mono_rect/compressed /kinect2/hd/camera_info /markersAruco /stt /tf /study_round 
        /cozmo/imu /cozmo/joint_states /empathy_channel /audio/audio /rosout_agg /study_out /diagnostics"
  />



</launch>